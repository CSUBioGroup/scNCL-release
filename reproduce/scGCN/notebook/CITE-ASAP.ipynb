{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "cur_dir = Path(os.getcwd())\n",
    "par_dir = cur_dir.parent.absolute()\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import scipy.sparse as sps\n",
    "\n",
    "sys.path.append(str(par_dir))\n",
    "from utils import *\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from models import scGCN\n",
    "# sys.stdout = open(str(cur_dir.joinpath('PBMC_lr=0.05_eps=50_outputs.txt')), \"w\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#' del_all_flags(FLAGS)\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x7f91a230e908>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Settings\n",
    "exp_id = 'CITE-ASAP'\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('f', '', 'kernel')  # to run in jupyter kernels\n",
    "flags.DEFINE_string('dataset', join(str(par_dir), f'input/{exp_id}'), 'data dir')\n",
    "flags.DEFINE_string('output', join(str(cur_dir), f'{exp_id}_results'), 'predicted results')\n",
    "flags.DEFINE_bool('graph', True, 'select the optional graph.')\n",
    "flags.DEFINE_string('model', 'scGCN','Model string.') \n",
    "flags.DEFINE_float('learning_rate', 0.05, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 32, 'Number of units in hidden layer 1.')\n",
    "#flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_float('dropout', 0, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 0,\n",
    "                   'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 10,\n",
    "                     'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
    "\n",
    "# output flow\n",
    "# sys.stdout = open(str(cur_dir.joinpath(f'{exp_id}_outputs.txt')), \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data succesfully....\n",
      "Constructing adjaceny graph\n",
      "assign input coordinatly....\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/models.py:94: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/models.py:40: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/utils.py:297: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/layers.py:68: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/layers.py:26: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/layers.py:33: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/layers.py:107: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/models.py:51: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/models.py:51: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/utils.py:268: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels_binary_train, labels_binary_val, labels_binary_test, train_mask, pred_mask, val_mask, test_mask, new_label, true_label, index_guide = load_data(\n",
    "    FLAGS.dataset,rgraph=FLAGS.graph)\n",
    "\n",
    "support = [preprocess_adj(adj)]\n",
    "num_supports = 1\n",
    "model_func = scGCN\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support':\n",
    "    [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features':\n",
    "    tf.sparse_placeholder(tf.float32,\n",
    "                          shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels':\n",
    "    tf.placeholder(tf.float32, shape=(None, labels_binary_train.shape[1])),\n",
    "    'labels_mask':\n",
    "    tf.placeholder(tf.int32),\n",
    "    'dropout':\n",
    "    tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero':\n",
    "    tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask,\n",
    "                                        placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.94592 train_acc= 0.07748 val_loss= 1.90444 val_acc= 0.31754 time= 2.62748\n",
      "Epoch: 0002 train_loss= 1.90377 train_acc= 0.31896 val_loss= 1.84623 val_acc= 0.31754 time= 2.00024\n",
      "Epoch: 0003 train_loss= 1.84446 train_acc= 0.31896 val_loss= 1.77988 val_acc= 0.31754 time= 2.00149\n",
      "Epoch: 0004 train_loss= 1.77655 train_acc= 0.31896 val_loss= 1.71894 val_acc= 0.31754 time= 1.97337\n",
      "Epoch: 0005 train_loss= 1.71319 train_acc= 0.31896 val_loss= 1.68072 val_acc= 0.31754 time= 2.03118\n",
      "Epoch: 0006 train_loss= 1.67106 train_acc= 0.31896 val_loss= 1.66413 val_acc= 0.31754 time= 2.05929\n",
      "Epoch: 0007 train_loss= 1.64935 train_acc= 0.31896 val_loss= 1.65168 val_acc= 0.31754 time= 1.98456\n",
      "Epoch: 0008 train_loss= 1.63152 train_acc= 0.31896 val_loss= 1.62931 val_acc= 0.31754 time= 2.05702\n",
      "Epoch: 0009 train_loss= 1.60461 train_acc= 0.31896 val_loss= 1.59602 val_acc= 0.31754 time= 1.96154\n",
      "Epoch: 0010 train_loss= 1.56822 train_acc= 0.31896 val_loss= 1.55936 val_acc= 0.35308 time= 1.96949\n",
      "Epoch: 0011 train_loss= 1.53000 train_acc= 0.36262 val_loss= 1.52529 val_acc= 0.41469 time= 1.95691\n",
      "Epoch: 0012 train_loss= 1.49578 train_acc= 0.41187 val_loss= 1.49474 val_acc= 0.42891 time= 2.04744\n",
      "Epoch: 0013 train_loss= 1.46597 train_acc= 0.44462 val_loss= 1.46458 val_acc= 0.47630 time= 1.99159\n",
      "Epoch: 0014 train_loss= 1.43656 train_acc= 0.49148 val_loss= 1.43149 val_acc= 0.50711 time= 1.96971\n",
      "Epoch: 0015 train_loss= 1.40336 train_acc= 0.52103 val_loss= 1.39527 val_acc= 0.51659 time= 2.02795\n",
      "Epoch: 0016 train_loss= 1.36566 train_acc= 0.53195 val_loss= 1.35861 val_acc= 0.51659 time= 2.02916\n",
      "Epoch: 0017 train_loss= 1.32597 train_acc= 0.53088 val_loss= 1.32489 val_acc= 0.50474 time= 2.03556\n",
      "Epoch: 0018 train_loss= 1.28819 train_acc= 0.52236 val_loss= 1.29628 val_acc= 0.48815 time= 1.99115\n",
      "Epoch: 0019 train_loss= 1.25489 train_acc= 0.51997 val_loss= 1.27053 val_acc= 0.50000 time= 2.04965\n",
      "Epoch: 0020 train_loss= 1.22489 train_acc= 0.53861 val_loss= 1.24350 val_acc= 0.50237 time= 1.98777\n",
      "Epoch: 0021 train_loss= 1.19503 train_acc= 0.54846 val_loss= 1.21262 val_acc= 0.52133 time= 2.01040\n",
      "Epoch: 0022 train_loss= 1.16325 train_acc= 0.56629 val_loss= 1.17860 val_acc= 0.54028 time= 1.95855\n",
      "Epoch: 0023 train_loss= 1.13017 train_acc= 0.58813 val_loss= 1.14408 val_acc= 0.56872 time= 2.00044\n",
      "Epoch: 0024 train_loss= 1.09771 train_acc= 0.60836 val_loss= 1.11109 val_acc= 0.59953 time= 1.96195\n",
      "Epoch: 0025 train_loss= 1.06698 train_acc= 0.62513 val_loss= 1.08006 val_acc= 0.62085 time= 1.94743\n",
      "Epoch: 0026 train_loss= 1.03759 train_acc= 0.63658 val_loss= 1.05022 val_acc= 0.63270 time= 1.94446\n",
      "Epoch: 0027 train_loss= 1.00810 train_acc= 0.64377 val_loss= 1.02014 val_acc= 0.64218 time= 1.91258\n",
      "Epoch: 0028 train_loss= 0.97717 train_acc= 0.65096 val_loss= 0.98945 val_acc= 0.64455 time= 1.97424\n",
      "Epoch: 0029 train_loss= 0.94488 train_acc= 0.65549 val_loss= 0.95919 val_acc= 0.64692 time= 1.96737\n",
      "Epoch: 0030 train_loss= 0.91279 train_acc= 0.65788 val_loss= 0.93011 val_acc= 0.65403 time= 1.95353\n",
      "Epoch: 0031 train_loss= 0.88220 train_acc= 0.66081 val_loss= 0.90123 val_acc= 0.65640 time= 1.91714\n",
      "Epoch: 0032 train_loss= 0.85258 train_acc= 0.66401 val_loss= 0.87102 val_acc= 0.66825 time= 2.00505\n",
      "Epoch: 0033 train_loss= 0.82262 train_acc= 0.67599 val_loss= 0.83939 val_acc= 0.70142 time= 1.97349\n",
      "Epoch: 0034 train_loss= 0.79229 train_acc= 0.70527 val_loss= 0.80793 val_acc= 0.73934 time= 1.88845\n",
      "Epoch: 0035 train_loss= 0.76286 train_acc= 0.74707 val_loss= 0.77770 val_acc= 0.77488 time= 1.88666\n",
      "Epoch: 0036 train_loss= 0.73497 train_acc= 0.78355 val_loss= 0.74837 val_acc= 0.79384 time= 1.93292\n",
      "Epoch: 0037 train_loss= 0.70780 train_acc= 0.79899 val_loss= 0.71986 val_acc= 0.81280 time= 2.02239\n",
      "Epoch: 0038 train_loss= 0.68069 train_acc= 0.82562 val_loss= 0.69277 val_acc= 0.81754 time= 1.98263\n",
      "Epoch: 0039 train_loss= 0.65391 train_acc= 0.83067 val_loss= 0.66758 val_acc= 0.81754 time= 1.92021\n",
      "Epoch: 0040 train_loss= 0.62818 train_acc= 0.83227 val_loss= 0.64356 val_acc= 0.82464 time= 1.96837\n",
      "Epoch: 0041 train_loss= 0.60350 train_acc= 0.84265 val_loss= 0.61933 val_acc= 0.84360 time= 1.90394\n",
      "Epoch: 0042 train_loss= 0.57915 train_acc= 0.85783 val_loss= 0.59487 val_acc= 0.86493 time= 1.97240\n",
      "Epoch: 0043 train_loss= 0.55520 train_acc= 0.87540 val_loss= 0.57140 val_acc= 0.87678 time= 1.94919\n",
      "Epoch: 0044 train_loss= 0.53245 train_acc= 0.89164 val_loss= 0.54932 val_acc= 0.88626 time= 1.99176\n",
      "Epoch: 0045 train_loss= 0.51078 train_acc= 0.89990 val_loss= 0.52829 val_acc= 0.89573 time= 1.92905\n",
      "Epoch: 0046 train_loss= 0.48954 train_acc= 0.90575 val_loss= 0.50842 val_acc= 0.90521 time= 1.94591\n",
      "Epoch: 0047 train_loss= 0.46897 train_acc= 0.91214 val_loss= 0.48984 val_acc= 0.91232 time= 1.95934\n",
      "Epoch: 0048 train_loss= 0.44966 train_acc= 0.91401 val_loss= 0.47193 val_acc= 0.91706 time= 1.97610\n",
      "Epoch: 0049 train_loss= 0.43136 train_acc= 0.91587 val_loss= 0.45421 val_acc= 0.92180 time= 1.90894\n",
      "Epoch: 0050 train_loss= 0.41366 train_acc= 0.92066 val_loss= 0.43726 val_acc= 0.92417 time= 1.97095\n",
      "Epoch: 0051 train_loss= 0.39692 train_acc= 0.92679 val_loss= 0.42173 val_acc= 0.93365 time= 2.06769\n",
      "Epoch: 0052 train_loss= 0.38141 train_acc= 0.92892 val_loss= 0.40745 val_acc= 0.93365 time= 2.02032\n",
      "Epoch: 0053 train_loss= 0.36672 train_acc= 0.92998 val_loss= 0.39430 val_acc= 0.93839 time= 1.93984\n",
      "Epoch: 0054 train_loss= 0.35268 train_acc= 0.93078 val_loss= 0.38233 val_acc= 0.93839 time= 1.93365\n",
      "Epoch: 0055 train_loss= 0.33961 train_acc= 0.93291 val_loss= 0.37106 val_acc= 0.93839 time= 1.93192\n",
      "Epoch: 0056 train_loss= 0.32738 train_acc= 0.93344 val_loss= 0.35997 val_acc= 0.94313 time= 2.05951\n",
      "Epoch: 0057 train_loss= 0.31574 train_acc= 0.93557 val_loss= 0.34927 val_acc= 0.94313 time= 1.92887\n",
      "Epoch: 0058 train_loss= 0.30482 train_acc= 0.93770 val_loss= 0.33943 val_acc= 0.93839 time= 1.94452\n",
      "Epoch: 0059 train_loss= 0.29472 train_acc= 0.94063 val_loss= 0.33058 val_acc= 0.93839 time= 2.04743\n",
      "Epoch: 0060 train_loss= 0.28516 train_acc= 0.94143 val_loss= 0.32270 val_acc= 0.94076 time= 1.93219\n",
      "Epoch: 0061 train_loss= 0.27610 train_acc= 0.94276 val_loss= 0.31567 val_acc= 0.94313 time= 1.92668\n",
      "Epoch: 0062 train_loss= 0.26767 train_acc= 0.94409 val_loss= 0.30900 val_acc= 0.94313 time= 2.00246\n",
      "Epoch: 0063 train_loss= 0.25981 train_acc= 0.94489 val_loss= 0.30243 val_acc= 0.94313 time= 1.95129\n",
      "Epoch: 0064 train_loss= 0.25235 train_acc= 0.94516 val_loss= 0.29617 val_acc= 0.94076 time= 1.96289\n",
      "Epoch: 0065 train_loss= 0.24534 train_acc= 0.94569 val_loss= 0.29055 val_acc= 0.94076 time= 1.99503\n",
      "Epoch: 0066 train_loss= 0.23880 train_acc= 0.94729 val_loss= 0.28556 val_acc= 0.94076 time= 1.98327\n",
      "Epoch: 0067 train_loss= 0.23261 train_acc= 0.94782 val_loss= 0.28103 val_acc= 0.94076 time= 1.91284\n",
      "Epoch: 0068 train_loss= 0.22673 train_acc= 0.94862 val_loss= 0.27676 val_acc= 0.94313 time= 1.97187\n",
      "Epoch: 0069 train_loss= 0.22118 train_acc= 0.94942 val_loss= 0.27260 val_acc= 0.94076 time= 2.01247\n",
      "Epoch: 0070 train_loss= 0.21595 train_acc= 0.94995 val_loss= 0.26846 val_acc= 0.94076 time= 2.00155\n",
      "Epoch: 0071 train_loss= 0.21096 train_acc= 0.95102 val_loss= 0.26442 val_acc= 0.94076 time= 1.92993\n",
      "Epoch: 0072 train_loss= 0.20619 train_acc= 0.95181 val_loss= 0.26063 val_acc= 0.94076 time= 2.04589\n",
      "Epoch: 0073 train_loss= 0.20168 train_acc= 0.95208 val_loss= 0.25714 val_acc= 0.94076 time= 1.94248\n",
      "Epoch: 0074 train_loss= 0.19738 train_acc= 0.95288 val_loss= 0.25398 val_acc= 0.94076 time= 1.98968\n",
      "Epoch: 0075 train_loss= 0.19324 train_acc= 0.95368 val_loss= 0.25113 val_acc= 0.94076 time= 1.96122\n",
      "Epoch: 0076 train_loss= 0.18928 train_acc= 0.95368 val_loss= 0.24850 val_acc= 0.94076 time= 1.97148\n",
      "Epoch: 0077 train_loss= 0.18550 train_acc= 0.95421 val_loss= 0.24589 val_acc= 0.94076 time= 1.94901\n",
      "Epoch: 0078 train_loss= 0.18187 train_acc= 0.95501 val_loss= 0.24321 val_acc= 0.94076 time= 1.99697\n",
      "Epoch: 0079 train_loss= 0.17836 train_acc= 0.95581 val_loss= 0.24049 val_acc= 0.94076 time= 1.99784\n",
      "Epoch: 0080 train_loss= 0.17499 train_acc= 0.95714 val_loss= 0.23784 val_acc= 0.94076 time= 1.98052\n",
      "Epoch: 0081 train_loss= 0.17174 train_acc= 0.95741 val_loss= 0.23533 val_acc= 0.94076 time= 1.97045\n",
      "Epoch: 0082 train_loss= 0.16859 train_acc= 0.95794 val_loss= 0.23297 val_acc= 0.94076 time= 1.97184\n",
      "Epoch: 0083 train_loss= 0.16555 train_acc= 0.95847 val_loss= 0.23073 val_acc= 0.94076 time= 1.98432\n",
      "Epoch: 0084 train_loss= 0.16260 train_acc= 0.95874 val_loss= 0.22859 val_acc= 0.94076 time= 1.94583\n",
      "Epoch: 0085 train_loss= 0.15974 train_acc= 0.95874 val_loss= 0.22657 val_acc= 0.94076 time= 1.96413\n",
      "Epoch: 0086 train_loss= 0.15697 train_acc= 0.95954 val_loss= 0.22469 val_acc= 0.94076 time= 1.96879\n",
      "Epoch: 0087 train_loss= 0.15426 train_acc= 0.95954 val_loss= 0.22292 val_acc= 0.94076 time= 1.92489\n",
      "Epoch: 0088 train_loss= 0.15164 train_acc= 0.96007 val_loss= 0.22118 val_acc= 0.94076 time= 1.97749\n",
      "Epoch: 0089 train_loss= 0.14909 train_acc= 0.96113 val_loss= 0.21942 val_acc= 0.94076 time= 1.94797\n",
      "Epoch: 0090 train_loss= 0.14661 train_acc= 0.96167 val_loss= 0.21767 val_acc= 0.94076 time= 2.01071\n",
      "Epoch: 0091 train_loss= 0.14418 train_acc= 0.96220 val_loss= 0.21597 val_acc= 0.94076 time= 1.96761\n",
      "Epoch: 0092 train_loss= 0.14182 train_acc= 0.96273 val_loss= 0.21432 val_acc= 0.94076 time= 1.93055\n",
      "Epoch: 0093 train_loss= 0.13951 train_acc= 0.96380 val_loss= 0.21271 val_acc= 0.94076 time= 1.92014\n",
      "Epoch: 0094 train_loss= 0.13725 train_acc= 0.96406 val_loss= 0.21117 val_acc= 0.94076 time= 1.96866\n",
      "Epoch: 0095 train_loss= 0.13505 train_acc= 0.96513 val_loss= 0.20970 val_acc= 0.94313 time= 1.96185\n",
      "Epoch: 0096 train_loss= 0.13290 train_acc= 0.96539 val_loss= 0.20834 val_acc= 0.94313 time= 1.96861\n",
      "Epoch: 0097 train_loss= 0.13080 train_acc= 0.96619 val_loss= 0.20706 val_acc= 0.94313 time= 1.94377\n",
      "Epoch: 0098 train_loss= 0.12874 train_acc= 0.96699 val_loss= 0.20581 val_acc= 0.94313 time= 1.96806\n",
      "Epoch: 0099 train_loss= 0.12672 train_acc= 0.96726 val_loss= 0.20457 val_acc= 0.94313 time= 1.98943\n",
      "Epoch: 0100 train_loss= 0.12475 train_acc= 0.96806 val_loss= 0.20333 val_acc= 0.94313 time= 1.96469\n",
      "Epoch: 0101 train_loss= 0.12282 train_acc= 0.96885 val_loss= 0.20211 val_acc= 0.94313 time= 1.92871\n",
      "Epoch: 0102 train_loss= 0.12092 train_acc= 0.96939 val_loss= 0.20091 val_acc= 0.94313 time= 1.96013\n",
      "Epoch: 0103 train_loss= 0.11907 train_acc= 0.96939 val_loss= 0.19974 val_acc= 0.94313 time= 1.94951\n",
      "Epoch: 0104 train_loss= 0.11726 train_acc= 0.97018 val_loss= 0.19859 val_acc= 0.94550 time= 1.90416\n",
      "Epoch: 0105 train_loss= 0.11548 train_acc= 0.97018 val_loss= 0.19748 val_acc= 0.94550 time= 1.93590\n",
      "Epoch: 0106 train_loss= 0.11373 train_acc= 0.97072 val_loss= 0.19641 val_acc= 0.95024 time= 2.02109\n",
      "Epoch: 0107 train_loss= 0.11202 train_acc= 0.97072 val_loss= 0.19540 val_acc= 0.95024 time= 2.01827\n",
      "Epoch: 0108 train_loss= 0.11035 train_acc= 0.97125 val_loss= 0.19443 val_acc= 0.95024 time= 1.92120\n",
      "Epoch: 0109 train_loss= 0.10870 train_acc= 0.97205 val_loss= 0.19349 val_acc= 0.95024 time= 1.96577\n",
      "Epoch: 0110 train_loss= 0.10709 train_acc= 0.97311 val_loss= 0.19256 val_acc= 0.95261 time= 1.97311\n",
      "Epoch: 0111 train_loss= 0.10551 train_acc= 0.97391 val_loss= 0.19164 val_acc= 0.95261 time= 2.01476\n",
      "Epoch: 0112 train_loss= 0.10396 train_acc= 0.97524 val_loss= 0.19075 val_acc= 0.95261 time= 1.98498\n",
      "Epoch: 0113 train_loss= 0.10245 train_acc= 0.97551 val_loss= 0.18987 val_acc= 0.95024 time= 2.00682\n",
      "Epoch: 0114 train_loss= 0.10096 train_acc= 0.97551 val_loss= 0.18902 val_acc= 0.95024 time= 1.94623\n",
      "Epoch: 0115 train_loss= 0.09950 train_acc= 0.97604 val_loss= 0.18819 val_acc= 0.95024 time= 1.99352\n",
      "Epoch: 0116 train_loss= 0.09807 train_acc= 0.97657 val_loss= 0.18739 val_acc= 0.95024 time= 1.95145\n",
      "Epoch: 0117 train_loss= 0.09666 train_acc= 0.97684 val_loss= 0.18663 val_acc= 0.95024 time= 1.94203\n",
      "Epoch: 0118 train_loss= 0.09529 train_acc= 0.97737 val_loss= 0.18591 val_acc= 0.95024 time= 1.90873\n",
      "Epoch: 0119 train_loss= 0.09394 train_acc= 0.97791 val_loss= 0.18521 val_acc= 0.95024 time= 1.95100\n",
      "Epoch: 0120 train_loss= 0.09262 train_acc= 0.97870 val_loss= 0.18452 val_acc= 0.95024 time= 1.96524\n",
      "Epoch: 0121 train_loss= 0.09132 train_acc= 0.97924 val_loss= 0.18384 val_acc= 0.95261 time= 1.90905\n",
      "Epoch: 0122 train_loss= 0.09005 train_acc= 0.97950 val_loss= 0.18318 val_acc= 0.95261 time= 1.89889\n",
      "Epoch: 0123 train_loss= 0.08880 train_acc= 0.97950 val_loss= 0.18254 val_acc= 0.95261 time= 1.94398\n",
      "Epoch: 0124 train_loss= 0.08758 train_acc= 0.97977 val_loss= 0.18191 val_acc= 0.95261 time= 1.95791\n",
      "Epoch: 0125 train_loss= 0.08638 train_acc= 0.97977 val_loss= 0.18129 val_acc= 0.95261 time= 1.97270\n",
      "Epoch: 0126 train_loss= 0.08520 train_acc= 0.98004 val_loss= 0.18068 val_acc= 0.95261 time= 1.98895\n",
      "Epoch: 0127 train_loss= 0.08405 train_acc= 0.98057 val_loss= 0.18010 val_acc= 0.95261 time= 1.99284\n",
      "Epoch: 0128 train_loss= 0.08291 train_acc= 0.98083 val_loss= 0.17955 val_acc= 0.95261 time= 1.92869\n",
      "Epoch: 0129 train_loss= 0.08180 train_acc= 0.98083 val_loss= 0.17902 val_acc= 0.95261 time= 1.99077\n",
      "Epoch: 0130 train_loss= 0.08071 train_acc= 0.98137 val_loss= 0.17850 val_acc= 0.95261 time= 1.93723\n",
      "Epoch: 0131 train_loss= 0.07964 train_acc= 0.98137 val_loss= 0.17799 val_acc= 0.95261 time= 1.96032\n",
      "Epoch: 0132 train_loss= 0.07858 train_acc= 0.98190 val_loss= 0.17750 val_acc= 0.95261 time= 1.94079\n",
      "Epoch: 0133 train_loss= 0.07755 train_acc= 0.98217 val_loss= 0.17703 val_acc= 0.95261 time= 1.93103\n",
      "Epoch: 0134 train_loss= 0.07653 train_acc= 0.98323 val_loss= 0.17658 val_acc= 0.95261 time= 1.98333\n",
      "Epoch: 0135 train_loss= 0.07554 train_acc= 0.98350 val_loss= 0.17613 val_acc= 0.95261 time= 1.94614\n",
      "Epoch: 0136 train_loss= 0.07456 train_acc= 0.98376 val_loss= 0.17570 val_acc= 0.95261 time= 2.01480\n",
      "Epoch: 0137 train_loss= 0.07360 train_acc= 0.98376 val_loss= 0.17529 val_acc= 0.95261 time= 1.98613\n",
      "Epoch: 0138 train_loss= 0.07266 train_acc= 0.98403 val_loss= 0.17490 val_acc= 0.95261 time= 1.92087\n",
      "Epoch: 0139 train_loss= 0.07173 train_acc= 0.98536 val_loss= 0.17453 val_acc= 0.95261 time= 1.91895\n",
      "Epoch: 0140 train_loss= 0.07082 train_acc= 0.98536 val_loss= 0.17418 val_acc= 0.95261 time= 1.92777\n",
      "Epoch: 0141 train_loss= 0.06992 train_acc= 0.98536 val_loss= 0.17384 val_acc= 0.95498 time= 1.90210\n",
      "Epoch: 0142 train_loss= 0.06904 train_acc= 0.98589 val_loss= 0.17353 val_acc= 0.95498 time= 1.90984\n",
      "Epoch: 0143 train_loss= 0.06818 train_acc= 0.98616 val_loss= 0.17323 val_acc= 0.95498 time= 1.99002\n",
      "Epoch: 0144 train_loss= 0.06733 train_acc= 0.98643 val_loss= 0.17295 val_acc= 0.95735 time= 1.95149\n",
      "Epoch: 0145 train_loss= 0.06649 train_acc= 0.98643 val_loss= 0.17268 val_acc= 0.95735 time= 1.88985\n",
      "Epoch: 0146 train_loss= 0.06567 train_acc= 0.98643 val_loss= 0.17243 val_acc= 0.95735 time= 1.91267\n",
      "Epoch: 0147 train_loss= 0.06487 train_acc= 0.98643 val_loss= 0.17218 val_acc= 0.95735 time= 2.01057\n",
      "Epoch: 0148 train_loss= 0.06407 train_acc= 0.98696 val_loss= 0.17194 val_acc= 0.95735 time= 1.95771\n",
      "Epoch: 0149 train_loss= 0.06329 train_acc= 0.98722 val_loss= 0.17171 val_acc= 0.95735 time= 1.96612\n",
      "Epoch: 0150 train_loss= 0.06252 train_acc= 0.98776 val_loss= 0.17151 val_acc= 0.95735 time= 1.95419\n",
      "Epoch: 0151 train_loss= 0.06177 train_acc= 0.98802 val_loss= 0.17131 val_acc= 0.95735 time= 2.01428\n",
      "Epoch: 0152 train_loss= 0.06103 train_acc= 0.98829 val_loss= 0.17112 val_acc= 0.95735 time= 1.93199\n",
      "Epoch: 0153 train_loss= 0.06030 train_acc= 0.98829 val_loss= 0.17095 val_acc= 0.95735 time= 1.94277\n",
      "Epoch: 0154 train_loss= 0.05958 train_acc= 0.98856 val_loss= 0.17079 val_acc= 0.95735 time= 1.90423\n",
      "Epoch: 0155 train_loss= 0.05887 train_acc= 0.98856 val_loss= 0.17063 val_acc= 0.95735 time= 1.97572\n",
      "Epoch: 0156 train_loss= 0.05817 train_acc= 0.98882 val_loss= 0.17049 val_acc= 0.95735 time= 2.02101\n",
      "Epoch: 0157 train_loss= 0.05749 train_acc= 0.98882 val_loss= 0.17035 val_acc= 0.95735 time= 1.97030\n",
      "Epoch: 0158 train_loss= 0.05682 train_acc= 0.98882 val_loss= 0.17021 val_acc= 0.95735 time= 1.94038\n",
      "Epoch: 0159 train_loss= 0.05615 train_acc= 0.98882 val_loss= 0.17009 val_acc= 0.95735 time= 1.93337\n",
      "Epoch: 0160 train_loss= 0.05550 train_acc= 0.98882 val_loss= 0.16999 val_acc= 0.95735 time= 1.96846\n",
      "Epoch: 0161 train_loss= 0.05486 train_acc= 0.98882 val_loss= 0.16989 val_acc= 0.95735 time= 1.92409\n",
      "Epoch: 0162 train_loss= 0.05422 train_acc= 0.98909 val_loss= 0.16980 val_acc= 0.95735 time= 1.96504\n",
      "Epoch: 0163 train_loss= 0.05360 train_acc= 0.98935 val_loss= 0.16972 val_acc= 0.95735 time= 2.02023\n",
      "Epoch: 0164 train_loss= 0.05299 train_acc= 0.98962 val_loss= 0.16965 val_acc= 0.95735 time= 2.01102\n",
      "Epoch: 0165 train_loss= 0.05238 train_acc= 0.99015 val_loss= 0.16960 val_acc= 0.95735 time= 1.92168\n",
      "Epoch: 0166 train_loss= 0.05179 train_acc= 0.99015 val_loss= 0.16955 val_acc= 0.95972 time= 1.94617\n",
      "Epoch: 0167 train_loss= 0.05120 train_acc= 0.99042 val_loss= 0.16951 val_acc= 0.95972 time= 1.96770\n",
      "Epoch: 0168 train_loss= 0.05062 train_acc= 0.99042 val_loss= 0.16947 val_acc= 0.95972 time= 2.02694\n",
      "Epoch: 0169 train_loss= 0.05006 train_acc= 0.99069 val_loss= 0.16944 val_acc= 0.95972 time= 1.89780\n",
      "Epoch: 0170 train_loss= 0.04950 train_acc= 0.99069 val_loss= 0.16943 val_acc= 0.95972 time= 1.97769\n",
      "Epoch: 0171 train_loss= 0.04894 train_acc= 0.99095 val_loss= 0.16942 val_acc= 0.95972 time= 1.93479\n",
      "Epoch: 0172 train_loss= 0.04840 train_acc= 0.99095 val_loss= 0.16941 val_acc= 0.95972 time= 1.93377\n",
      "Epoch: 0173 train_loss= 0.04786 train_acc= 0.99095 val_loss= 0.16941 val_acc= 0.95972 time= 1.89757\n",
      "Epoch: 0174 train_loss= 0.04733 train_acc= 0.99148 val_loss= 0.16942 val_acc= 0.95972 time= 1.90990\n",
      "Epoch: 0175 train_loss= 0.04681 train_acc= 0.99175 val_loss= 0.16944 val_acc= 0.95972 time= 1.95188\n",
      "Epoch: 0176 train_loss= 0.04630 train_acc= 0.99175 val_loss= 0.16947 val_acc= 0.95972 time= 1.94546\n",
      "Early stopping...\n",
      "Finished Training....\n"
     ]
    }
   ],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_accuracy = []\n",
    "train_loss = []\n",
    "val_accuracy = []\n",
    "val_loss = []\n",
    "test_accuracy = []\n",
    "test_loss = []\n",
    "\n",
    "# Train model\n",
    "\n",
    "#configurate checkpoint directory to save intermediate model training weights\n",
    "saver = tf.train.Saver()\n",
    "save_dir = str(cur_dir.joinpath(f'{exp_id}_checkpoints/'))\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "save_path = os.path.join(save_dir, 'best_validation')\n",
    "\n",
    "for epoch in range(FLAGS.epochs):\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, labels_binary_train,\n",
    "                                    train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy],\n",
    "                    feed_dict=feed_dict)\n",
    "    train_accuracy.append(outs[2])\n",
    "    train_loss.append(outs[1])\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, labels_binary_val,\n",
    "                                   val_mask, placeholders)\n",
    "    val_loss.append(cost)\n",
    "    val_accuracy.append(acc)\n",
    "    test_cost, test_acc, test_duration = evaluate(features, support,\n",
    "                                                  labels_binary_test,\n",
    "                                                  test_mask, placeholders)\n",
    "    test_accuracy.append(test_acc)\n",
    "    test_loss.append(test_cost)\n",
    "    saver.save(sess=sess, save_path=save_path)\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\",\n",
    "          \"{:.5f}\".format(outs[1]), \"train_acc=\", \"{:.5f}\".format(outs[2]),\n",
    "          \"val_loss=\", \"{:.5f}\".format(cost), \"val_acc=\", \"{:.5f}\".format(acc),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "    if epoch > FLAGS.early_stopping and val_loss[-1] > np.mean(\n",
    "            val_loss[-(FLAGS.early_stopping + 1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Finished Training....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking train/test/val set accuracy: 0.9917465388711395, 0.9506437768240343, 0.9597156398104265\n",
      "Checking pred set accuracy: 0.832741003998223\n"
     ]
    }
   ],
   "source": [
    "all_mask = np.array([True] * len(train_mask))\n",
    "labels_binary_all = new_label\n",
    "\n",
    "feed_dict_all = construct_feed_dict(features, support, labels_binary_all,\n",
    "                                    all_mask, placeholders)\n",
    "feed_dict_all.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "activation_output = sess.run(model.activations, feed_dict=feed_dict_all)[1]\n",
    "predict_output = sess.run(model.outputs, feed_dict=feed_dict_all)\n",
    "\n",
    "#' accuracy on all masks\n",
    "ab = sess.run(tf.nn.softmax(predict_output))\n",
    "all_prediction = sess.run(\n",
    "    tf.equal(sess.run(tf.argmax(ab, 1)),\n",
    "             sess.run(tf.argmax(labels_binary_all.astype(\"int32\"), 1))))\n",
    "\n",
    "#' accuracy on prediction masks \n",
    "acc_train = np.sum(all_prediction[train_mask]) / np.sum(train_mask)\n",
    "acc_test = np.sum(all_prediction[test_mask]) / np.sum(test_mask)\n",
    "acc_val = np.sum(all_prediction[val_mask]) / np.sum(val_mask)\n",
    "acc_pred = np.sum(all_prediction[pred_mask]) / np.sum(pred_mask)\n",
    "print('Checking train/test/val set accuracy: {}, {}, {}'.format(\n",
    "    acc_train, acc_test, acc_val))\n",
    "print('Checking pred set accuracy: {}'.format(acc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.832741003998223, 0.9917465388711395, 0.9597156398104265)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_pred, acc_train, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_label = pd.read_csv(join(FLAGS.dataset, 'Label1.csv'))['type'].values\n",
    "tgt_label = pd.read_csv(join(FLAGS.dataset, 'Label2.csv'))['type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_shr_mask = np.in1d(tgt_label, np.unique(src_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pr = np.argmax(ab, axis=1)\n",
    "all_gt = np.argmax(labels_binary_all.A, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8903055087803705"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_pr[pred_mask][tgt_shr_mask] == all_gt[pred_mask][tgt_shr_mask]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "close_acc= 0.8903\n",
      "AUROC= 0.6550\n",
      "AUPR= 0.1203\n",
      "OSCR= 0.6657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8903055087803705,\n",
       " 0.6550159849110807,\n",
       " 0.1202544722704429,\n",
       " 0.6657232605732256)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metrics import osr_evaluator\n",
    "\n",
    "E_score = pd.read_csv(join(FLAGS.dataset, 'sample_E_score.csv'))\n",
    "H_score = pd.read_csv(join(FLAGS.dataset, 'sample_H_score.csv'))\n",
    "\n",
    "E_score = E_score.x.replace(float('-inf'), 0).values\n",
    "H_score = H_score.x.replace(float('-inf'), 0).values\n",
    "\n",
    "open_score = H_score - E_score\n",
    "\n",
    "kn_data_pr = all_pr[pred_mask][tgt_shr_mask]\n",
    "kn_data_gt = all_gt[pred_mask][tgt_shr_mask]\n",
    "kn_data_open_score = open_score[tgt_shr_mask]\n",
    "\n",
    "unk_data_open_score = open_score[np.logical_not(tgt_shr_mask)]\n",
    "\n",
    "closed_acc, os_auroc, os_aupr, oscr = osr_evaluator(kn_data_pr, kn_data_gt, kn_data_open_score, unk_data_open_score)\n",
    "closed_acc, os_auroc, os_aupr, oscr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf113",
   "language": "python",
   "name": "tf113"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
