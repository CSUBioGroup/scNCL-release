{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "cur_dir = Path(os.getcwd())\n",
    "par_dir = cur_dir.parent.absolute()\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import scipy.sparse as sps\n",
    "\n",
    "sys.path.append(str(par_dir))\n",
    "from utils import *\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from models import scGCN\n",
    "# sys.stdout = open(str(cur_dir.joinpath('PBMC_lr=0.05_eps=50_outputs.txt')), \"w\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#' del_all_flags(FLAGS)\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x7f4741abe6d8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Settings\n",
    "exp_id = 'PBMCMultome'\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('f', '', 'kernel')  # to run in jupyter kernels\n",
    "flags.DEFINE_string('dataset', join(str(par_dir), f'input/{exp_id}'), 'data dir')\n",
    "flags.DEFINE_string('output', join(str(cur_dir), f'{exp_id}_results'), 'predicted results')\n",
    "flags.DEFINE_bool('graph', True, 'select the optional graph.')\n",
    "flags.DEFINE_string('model', 'scGCN','Model string.') \n",
    "flags.DEFINE_float('learning_rate', 0.05, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 32, 'Number of units in hidden layer 1.')\n",
    "#flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_float('dropout', 0, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 0,\n",
    "                   'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 10,\n",
    "                     'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
    "\n",
    "# output flow\n",
    "# sys.stdout = open(str(cur_dir.joinpath(f'{exp_id}_outputs.txt')), \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data succesfully....\n",
      "Constructing adjaceny graph\n",
      "assign input coordinatly....\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/models.py:94: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/models.py:40: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/utils.py:297: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/layers.py:68: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/layers.py:26: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/layers.py:33: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/layers.py:107: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/models.py:51: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/models.py:51: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yxh/gitrepo/multi-omics/scGCN/scGCN/utils.py:268: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels_binary_train, labels_binary_val, labels_binary_test, train_mask, pred_mask, val_mask, test_mask, new_label, true_label, index_guide = load_data(\n",
    "    FLAGS.dataset,rgraph=FLAGS.graph)\n",
    "\n",
    "support = [preprocess_adj(adj)]\n",
    "num_supports = 1\n",
    "model_func = scGCN\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support':\n",
    "    [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features':\n",
    "    tf.sparse_placeholder(tf.float32,\n",
    "                          shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels':\n",
    "    tf.placeholder(tf.float32, shape=(None, labels_binary_train.shape[1])),\n",
    "    'labels_mask':\n",
    "    tf.placeholder(tf.int32),\n",
    "    'dropout':\n",
    "    tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero':\n",
    "    tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask,\n",
    "                                        placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 2.94502 train_acc= 0.03671 val_loss= 2.89505 val_acc= 0.20296 time= 5.31681\n",
      "Epoch: 0002 train_loss= 2.89443 train_acc= 0.20969 val_loss= 2.80472 val_acc= 0.36047 time= 5.15910\n",
      "Epoch: 0003 train_loss= 2.80296 train_acc= 0.36391 val_loss= 2.67659 val_acc= 0.36786 time= 5.11583\n",
      "Epoch: 0004 train_loss= 2.67318 train_acc= 0.37270 val_loss= 2.52721 val_acc= 0.36998 time= 5.14389\n",
      "Epoch: 0005 train_loss= 2.52147 train_acc= 0.37353 val_loss= 2.38548 val_acc= 0.37209 time= 5.18714\n",
      "Epoch: 0006 train_loss= 2.37675 train_acc= 0.37710 val_loss= 2.28128 val_acc= 0.44503 time= 5.38335\n",
      "Epoch: 0007 train_loss= 2.26915 train_acc= 0.45123 val_loss= 2.21287 val_acc= 0.40275 time= 5.35511\n",
      "Epoch: 0008 train_loss= 2.19745 train_acc= 0.40608 val_loss= 2.15572 val_acc= 0.40169 time= 5.16486\n",
      "Epoch: 0009 train_loss= 2.13771 train_acc= 0.40264 val_loss= 2.09761 val_acc= 0.39218 time= 5.33484\n",
      "Epoch: 0010 train_loss= 2.07788 train_acc= 0.39765 val_loss= 2.03505 val_acc= 0.40381 time= 5.32623\n",
      "Epoch: 0011 train_loss= 2.01464 train_acc= 0.40204 val_loss= 1.96700 val_acc= 0.41649 time= 5.13842\n",
      "Epoch: 0012 train_loss= 1.94675 train_acc= 0.41202 val_loss= 1.89826 val_acc= 0.43340 time= 5.12794\n",
      "Epoch: 0013 train_loss= 1.87877 train_acc= 0.42521 val_loss= 1.83641 val_acc= 0.43975 time= 5.16813\n",
      "Epoch: 0014 train_loss= 1.81786 train_acc= 0.43305 val_loss= 1.78560 val_acc= 0.43975 time= 5.21523\n",
      "Epoch: 0015 train_loss= 1.76795 train_acc= 0.43650 val_loss= 1.74567 val_acc= 0.43869 time= 5.21602\n",
      "Epoch: 0016 train_loss= 1.72874 train_acc= 0.43579 val_loss= 1.71360 val_acc= 0.43763 time= 5.14355\n",
      "Epoch: 0017 train_loss= 1.69711 train_acc= 0.43579 val_loss= 1.68573 val_acc= 0.44080 time= 5.23078\n",
      "Epoch: 0018 train_loss= 1.66934 train_acc= 0.43864 val_loss= 1.65975 val_acc= 0.44080 time= 5.25835\n",
      "Epoch: 0019 train_loss= 1.64338 train_acc= 0.44291 val_loss= 1.63524 val_acc= 0.45455 time= 5.34243\n",
      "Epoch: 0020 train_loss= 1.61883 train_acc= 0.45361 val_loss= 1.61198 val_acc= 0.47146 time= 5.09294\n",
      "Epoch: 0021 train_loss= 1.59558 train_acc= 0.47083 val_loss= 1.58945 val_acc= 0.48203 time= 5.12174\n",
      "Epoch: 0022 train_loss= 1.57313 train_acc= 0.48996 val_loss= 1.56702 val_acc= 0.49049 time= 5.14316\n",
      "Epoch: 0023 train_loss= 1.55090 train_acc= 0.50303 val_loss= 1.54464 val_acc= 0.49789 time= 5.17486\n",
      "Epoch: 0024 train_loss= 1.52885 train_acc= 0.51028 val_loss= 1.52281 val_acc= 0.50529 time= 5.33285\n",
      "Epoch: 0025 train_loss= 1.50737 train_acc= 0.51325 val_loss= 1.50165 val_acc= 0.51374 time= 5.17259\n",
      "Epoch: 0026 train_loss= 1.48651 train_acc= 0.52204 val_loss= 1.48054 val_acc= 0.53277 time= 5.40501\n",
      "Epoch: 0027 train_loss= 1.46562 train_acc= 0.53808 val_loss= 1.45882 val_acc= 0.55074 time= 5.22173\n",
      "Epoch: 0028 train_loss= 1.44402 train_acc= 0.55020 val_loss= 1.43612 val_acc= 0.55920 time= 5.14411\n",
      "Epoch: 0029 train_loss= 1.42131 train_acc= 0.56136 val_loss= 1.41252 val_acc= 0.56448 time= 5.19132\n",
      "Epoch: 0030 train_loss= 1.39753 train_acc= 0.56897 val_loss= 1.38809 val_acc= 0.56343 time= 5.12520\n",
      "Epoch: 0031 train_loss= 1.37282 train_acc= 0.57360 val_loss= 1.36298 val_acc= 0.55814 time= 5.24149\n",
      "Epoch: 0032 train_loss= 1.34733 train_acc= 0.57657 val_loss= 1.33734 val_acc= 0.56237 time= 5.13768\n",
      "Epoch: 0033 train_loss= 1.32124 train_acc= 0.58156 val_loss= 1.31138 val_acc= 0.57611 time= 5.15315\n",
      "Epoch: 0034 train_loss= 1.29486 train_acc= 0.59297 val_loss= 1.28559 val_acc= 0.59408 time= 5.08057\n",
      "Epoch: 0035 train_loss= 1.26860 train_acc= 0.60413 val_loss= 1.26015 val_acc= 0.60465 time= 5.14107\n",
      "Epoch: 0036 train_loss= 1.24267 train_acc= 0.61697 val_loss= 1.23493 val_acc= 0.61522 time= 5.19795\n",
      "Epoch: 0037 train_loss= 1.21696 train_acc= 0.62671 val_loss= 1.20990 val_acc= 0.62368 time= 5.49819\n",
      "Epoch: 0038 train_loss= 1.19135 train_acc= 0.63289 val_loss= 1.18519 val_acc= 0.62791 time= 6.16990\n",
      "Epoch: 0039 train_loss= 1.16596 train_acc= 0.63847 val_loss= 1.16107 val_acc= 0.63002 time= 6.17580\n",
      "Epoch: 0040 train_loss= 1.14116 train_acc= 0.64275 val_loss= 1.13761 val_acc= 0.63531 time= 6.03396\n",
      "Epoch: 0041 train_loss= 1.11702 train_acc= 0.64833 val_loss= 1.11470 val_acc= 0.65011 time= 6.16268\n",
      "Epoch: 0042 train_loss= 1.09350 train_acc= 0.65653 val_loss= 1.09236 val_acc= 0.65751 time= 6.03427\n",
      "Epoch: 0043 train_loss= 1.07063 train_acc= 0.66983 val_loss= 1.07069 val_acc= 0.66913 time= 5.90572\n",
      "Epoch: 0044 train_loss= 1.04846 train_acc= 0.68397 val_loss= 1.04970 val_acc= 0.68605 time= 6.17838\n",
      "Epoch: 0045 train_loss= 1.02693 train_acc= 0.69478 val_loss= 1.02926 val_acc= 0.70613 time= 6.16275\n",
      "Epoch: 0046 train_loss= 1.00590 train_acc= 0.70524 val_loss= 1.00934 val_acc= 0.70930 time= 6.14725\n",
      "Epoch: 0047 train_loss= 0.98539 train_acc= 0.71272 val_loss= 0.98995 val_acc= 0.71353 time= 6.15398\n",
      "Epoch: 0048 train_loss= 0.96546 train_acc= 0.71938 val_loss= 0.97094 val_acc= 0.72199 time= 6.07864\n",
      "Epoch: 0049 train_loss= 0.94603 train_acc= 0.72532 val_loss= 0.95222 val_acc= 0.73150 time= 6.26430\n",
      "Epoch: 0050 train_loss= 0.92700 train_acc= 0.73221 val_loss= 0.93385 val_acc= 0.74419 time= 5.92348\n",
      "Epoch: 0051 train_loss= 0.90835 train_acc= 0.74231 val_loss= 0.91585 val_acc= 0.75793 time= 5.76442\n",
      "Epoch: 0052 train_loss= 0.89008 train_acc= 0.75217 val_loss= 0.89820 val_acc= 0.76321 time= 5.13516\n",
      "Epoch: 0053 train_loss= 0.87211 train_acc= 0.75989 val_loss= 0.88095 val_acc= 0.76850 time= 5.13673\n",
      "Epoch: 0054 train_loss= 0.85441 train_acc= 0.76975 val_loss= 0.86414 val_acc= 0.77167 time= 5.18551\n",
      "Epoch: 0055 train_loss= 0.83710 train_acc= 0.77747 val_loss= 0.84771 val_acc= 0.78118 time= 5.21513\n",
      "Epoch: 0056 train_loss= 0.82019 train_acc= 0.78448 val_loss= 0.83170 val_acc= 0.78964 time= 5.27246\n",
      "Epoch: 0057 train_loss= 0.80371 train_acc= 0.79137 val_loss= 0.81613 val_acc= 0.79387 time= 5.31450\n",
      "Epoch: 0058 train_loss= 0.78771 train_acc= 0.79553 val_loss= 0.80103 val_acc= 0.79915 time= 5.30178\n",
      "Epoch: 0059 train_loss= 0.77220 train_acc= 0.80016 val_loss= 0.78634 val_acc= 0.80233 time= 5.14232\n",
      "Epoch: 0060 train_loss= 0.75708 train_acc= 0.80468 val_loss= 0.77202 val_acc= 0.80655 time= 5.13778\n",
      "Epoch: 0061 train_loss= 0.74231 train_acc= 0.80836 val_loss= 0.75801 val_acc= 0.80550 time= 5.21531\n",
      "Epoch: 0062 train_loss= 0.72788 train_acc= 0.81050 val_loss= 0.74426 val_acc= 0.80973 time= 5.13550\n",
      "Epoch: 0063 train_loss= 0.71372 train_acc= 0.81311 val_loss= 0.73082 val_acc= 0.81290 time= 5.16098\n",
      "Epoch: 0064 train_loss= 0.69985 train_acc= 0.81620 val_loss= 0.71780 val_acc= 0.81184 time= 5.16357\n",
      "Epoch: 0065 train_loss= 0.68637 train_acc= 0.81965 val_loss= 0.70524 val_acc= 0.81607 time= 5.15292\n",
      "Epoch: 0066 train_loss= 0.67328 train_acc= 0.82214 val_loss= 0.69308 val_acc= 0.81713 time= 5.17296\n",
      "Epoch: 0067 train_loss= 0.66054 train_acc= 0.82523 val_loss= 0.68121 val_acc= 0.81607 time= 5.32045\n",
      "Epoch: 0068 train_loss= 0.64807 train_acc= 0.82642 val_loss= 0.66952 val_acc= 0.81607 time= 5.11787\n",
      "Epoch: 0069 train_loss= 0.63582 train_acc= 0.82797 val_loss= 0.65799 val_acc= 0.81924 time= 5.09827\n",
      "Epoch: 0070 train_loss= 0.62378 train_acc= 0.83022 val_loss= 0.64672 val_acc= 0.81924 time= 5.11771\n",
      "Epoch: 0071 train_loss= 0.61202 train_acc= 0.83319 val_loss= 0.63579 val_acc= 0.82347 time= 5.16101\n",
      "Epoch: 0072 train_loss= 0.60059 train_acc= 0.83486 val_loss= 0.62516 val_acc= 0.82770 time= 5.09130\n",
      "Epoch: 0073 train_loss= 0.58948 train_acc= 0.83557 val_loss= 0.61482 val_acc= 0.82875 time= 5.17613\n",
      "Epoch: 0074 train_loss= 0.57867 train_acc= 0.83652 val_loss= 0.60474 val_acc= 0.82875 time= 5.15704\n",
      "Epoch: 0075 train_loss= 0.56812 train_acc= 0.83795 val_loss= 0.59491 val_acc= 0.83192 time= 5.13409\n",
      "Epoch: 0076 train_loss= 0.55782 train_acc= 0.83997 val_loss= 0.58532 val_acc= 0.83192 time= 5.39516\n",
      "Epoch: 0077 train_loss= 0.54776 train_acc= 0.84163 val_loss= 0.57600 val_acc= 0.83721 time= 5.49801\n",
      "Epoch: 0078 train_loss= 0.53798 train_acc= 0.84341 val_loss= 0.56690 val_acc= 0.83827 time= 5.10894\n",
      "Epoch: 0079 train_loss= 0.52846 train_acc= 0.84602 val_loss= 0.55804 val_acc= 0.84038 time= 5.82821\n",
      "Epoch: 0080 train_loss= 0.51920 train_acc= 0.84697 val_loss= 0.54942 val_acc= 0.84038 time= 6.03129\n",
      "Epoch: 0081 train_loss= 0.51017 train_acc= 0.84816 val_loss= 0.54103 val_acc= 0.84144 time= 6.33704\n",
      "Epoch: 0082 train_loss= 0.50137 train_acc= 0.84971 val_loss= 0.53286 val_acc= 0.84355 time= 6.23594\n",
      "Epoch: 0083 train_loss= 0.49279 train_acc= 0.85090 val_loss= 0.52490 val_acc= 0.84461 time= 6.31945\n",
      "Epoch: 0084 train_loss= 0.48443 train_acc= 0.85220 val_loss= 0.51720 val_acc= 0.84778 time= 6.36306\n",
      "Epoch: 0085 train_loss= 0.47629 train_acc= 0.85339 val_loss= 0.50973 val_acc= 0.84884 time= 6.24769\n",
      "Epoch: 0086 train_loss= 0.46835 train_acc= 0.85517 val_loss= 0.50245 val_acc= 0.84778 time= 6.20188\n",
      "Epoch: 0087 train_loss= 0.46061 train_acc= 0.85660 val_loss= 0.49536 val_acc= 0.84461 time= 6.09484\n",
      "Epoch: 0088 train_loss= 0.45306 train_acc= 0.85921 val_loss= 0.48847 val_acc= 0.84672 time= 6.05448\n",
      "Epoch: 0089 train_loss= 0.44572 train_acc= 0.86099 val_loss= 0.48177 val_acc= 0.84884 time= 6.14474\n",
      "Epoch: 0090 train_loss= 0.43857 train_acc= 0.86171 val_loss= 0.47526 val_acc= 0.84989 time= 6.10108\n",
      "Epoch: 0091 train_loss= 0.43160 train_acc= 0.86361 val_loss= 0.46892 val_acc= 0.85095 time= 6.12709\n",
      "Epoch: 0092 train_loss= 0.42480 train_acc= 0.86539 val_loss= 0.46275 val_acc= 0.85518 time= 6.23720\n",
      "Epoch: 0093 train_loss= 0.41816 train_acc= 0.86693 val_loss= 0.45675 val_acc= 0.85518 time= 5.76090\n",
      "Epoch: 0094 train_loss= 0.41170 train_acc= 0.86860 val_loss= 0.45095 val_acc= 0.85835 time= 5.17990\n",
      "Epoch: 0095 train_loss= 0.40539 train_acc= 0.87062 val_loss= 0.44537 val_acc= 0.85835 time= 5.22421\n",
      "Epoch: 0096 train_loss= 0.39923 train_acc= 0.87169 val_loss= 0.43997 val_acc= 0.86258 time= 5.17982\n",
      "Epoch: 0097 train_loss= 0.39322 train_acc= 0.87264 val_loss= 0.43472 val_acc= 0.86364 time= 5.18600\n",
      "Epoch: 0098 train_loss= 0.38734 train_acc= 0.87442 val_loss= 0.42962 val_acc= 0.86469 time= 5.17281\n",
      "Epoch: 0099 train_loss= 0.38160 train_acc= 0.87620 val_loss= 0.42466 val_acc= 0.86681 time= 5.13870\n",
      "Epoch: 0100 train_loss= 0.37599 train_acc= 0.87775 val_loss= 0.41981 val_acc= 0.86681 time= 5.10581\n",
      "Epoch: 0101 train_loss= 0.37049 train_acc= 0.87953 val_loss= 0.41507 val_acc= 0.86787 time= 5.16487\n",
      "Epoch: 0102 train_loss= 0.36511 train_acc= 0.88095 val_loss= 0.41046 val_acc= 0.86787 time= 5.16775\n",
      "Epoch: 0103 train_loss= 0.35984 train_acc= 0.88285 val_loss= 0.40597 val_acc= 0.86892 time= 5.49030\n",
      "Epoch: 0104 train_loss= 0.35468 train_acc= 0.88523 val_loss= 0.40161 val_acc= 0.86998 time= 6.08212\n",
      "Epoch: 0105 train_loss= 0.34962 train_acc= 0.88737 val_loss= 0.39738 val_acc= 0.86998 time= 6.15972\n",
      "Epoch: 0106 train_loss= 0.34467 train_acc= 0.88832 val_loss= 0.39327 val_acc= 0.87104 time= 6.01893\n",
      "Epoch: 0107 train_loss= 0.33981 train_acc= 0.88939 val_loss= 0.38928 val_acc= 0.87315 time= 6.04624\n",
      "Epoch: 0108 train_loss= 0.33503 train_acc= 0.89165 val_loss= 0.38540 val_acc= 0.87421 time= 6.19206\n",
      "Epoch: 0109 train_loss= 0.33034 train_acc= 0.89331 val_loss= 0.38161 val_acc= 0.87526 time= 6.29074\n",
      "Epoch: 0110 train_loss= 0.32573 train_acc= 0.89450 val_loss= 0.37793 val_acc= 0.87738 time= 6.40222\n",
      "Epoch: 0111 train_loss= 0.32120 train_acc= 0.89640 val_loss= 0.37432 val_acc= 0.87738 time= 6.09317\n",
      "Epoch: 0112 train_loss= 0.31674 train_acc= 0.89818 val_loss= 0.37078 val_acc= 0.87844 time= 6.33737\n",
      "Epoch: 0113 train_loss= 0.31234 train_acc= 0.89973 val_loss= 0.36731 val_acc= 0.87949 time= 6.10954\n",
      "Epoch: 0114 train_loss= 0.30802 train_acc= 0.90068 val_loss= 0.36391 val_acc= 0.87844 time= 5.97131\n",
      "Epoch: 0115 train_loss= 0.30375 train_acc= 0.90151 val_loss= 0.36060 val_acc= 0.87844 time= 6.32267\n",
      "Epoch: 0116 train_loss= 0.29955 train_acc= 0.90246 val_loss= 0.35736 val_acc= 0.87949 time= 6.19897\n",
      "Epoch: 0117 train_loss= 0.29541 train_acc= 0.90317 val_loss= 0.35420 val_acc= 0.88055 time= 6.18117\n",
      "Epoch: 0118 train_loss= 0.29133 train_acc= 0.90472 val_loss= 0.35111 val_acc= 0.88266 time= 6.05739\n",
      "Epoch: 0119 train_loss= 0.28731 train_acc= 0.90685 val_loss= 0.34809 val_acc= 0.88478 time= 6.17935\n",
      "Epoch: 0120 train_loss= 0.28335 train_acc= 0.90769 val_loss= 0.34513 val_acc= 0.88689 time= 6.07714\n",
      "Epoch: 0121 train_loss= 0.27944 train_acc= 0.90935 val_loss= 0.34224 val_acc= 0.88795 time= 6.31999\n",
      "Epoch: 0122 train_loss= 0.27558 train_acc= 0.91125 val_loss= 0.33939 val_acc= 0.89006 time= 6.14963\n",
      "Epoch: 0123 train_loss= 0.27178 train_acc= 0.91339 val_loss= 0.33660 val_acc= 0.89006 time= 5.46588\n",
      "Epoch: 0124 train_loss= 0.26804 train_acc= 0.91565 val_loss= 0.33386 val_acc= 0.89006 time= 5.15171\n",
      "Epoch: 0125 train_loss= 0.26434 train_acc= 0.91671 val_loss= 0.33117 val_acc= 0.89324 time= 5.14530\n",
      "Epoch: 0126 train_loss= 0.26070 train_acc= 0.91802 val_loss= 0.32853 val_acc= 0.89218 time= 5.13629\n",
      "Epoch: 0127 train_loss= 0.25712 train_acc= 0.92004 val_loss= 0.32594 val_acc= 0.89324 time= 5.24404\n",
      "Epoch: 0128 train_loss= 0.25358 train_acc= 0.92147 val_loss= 0.32339 val_acc= 0.89324 time= 5.22746\n",
      "Epoch: 0129 train_loss= 0.25010 train_acc= 0.92254 val_loss= 0.32090 val_acc= 0.89324 time= 5.32509\n",
      "Epoch: 0130 train_loss= 0.24667 train_acc= 0.92444 val_loss= 0.31847 val_acc= 0.89429 time= 5.93442\n",
      "Epoch: 0131 train_loss= 0.24329 train_acc= 0.92586 val_loss= 0.31609 val_acc= 0.89852 time= 6.02103\n",
      "Epoch: 0132 train_loss= 0.23996 train_acc= 0.92800 val_loss= 0.31376 val_acc= 0.90063 time= 6.03948\n",
      "Epoch: 0133 train_loss= 0.23667 train_acc= 0.92978 val_loss= 0.31148 val_acc= 0.90063 time= 6.12653\n",
      "Epoch: 0134 train_loss= 0.23344 train_acc= 0.93085 val_loss= 0.30923 val_acc= 0.90063 time= 6.38903\n",
      "Epoch: 0135 train_loss= 0.23026 train_acc= 0.93228 val_loss= 0.30703 val_acc= 0.90063 time= 5.98482\n",
      "Epoch: 0136 train_loss= 0.22713 train_acc= 0.93311 val_loss= 0.30488 val_acc= 0.90063 time= 5.91482\n",
      "Epoch: 0137 train_loss= 0.22405 train_acc= 0.93513 val_loss= 0.30278 val_acc= 0.90063 time= 6.09983\n",
      "Epoch: 0138 train_loss= 0.22101 train_acc= 0.93620 val_loss= 0.30071 val_acc= 0.90063 time= 6.05436\n",
      "Epoch: 0139 train_loss= 0.21802 train_acc= 0.93715 val_loss= 0.29869 val_acc= 0.90275 time= 6.18321\n",
      "Epoch: 0140 train_loss= 0.21508 train_acc= 0.93798 val_loss= 0.29670 val_acc= 0.90486 time= 5.99264\n",
      "Epoch: 0141 train_loss= 0.21219 train_acc= 0.93893 val_loss= 0.29476 val_acc= 0.90486 time= 5.29251\n",
      "Epoch: 0142 train_loss= 0.20934 train_acc= 0.93988 val_loss= 0.29286 val_acc= 0.90486 time= 5.24087\n",
      "Epoch: 0143 train_loss= 0.20654 train_acc= 0.94095 val_loss= 0.29100 val_acc= 0.90486 time= 5.22035\n",
      "Epoch: 0144 train_loss= 0.20379 train_acc= 0.94226 val_loss= 0.28918 val_acc= 0.90486 time= 5.20563\n",
      "Epoch: 0145 train_loss= 0.20108 train_acc= 0.94297 val_loss= 0.28742 val_acc= 0.90592 time= 5.16411\n",
      "Epoch: 0146 train_loss= 0.19841 train_acc= 0.94404 val_loss= 0.28569 val_acc= 0.90592 time= 5.27154\n",
      "Epoch: 0147 train_loss= 0.19580 train_acc= 0.94547 val_loss= 0.28401 val_acc= 0.90592 time= 5.28430\n",
      "Epoch: 0148 train_loss= 0.19322 train_acc= 0.94701 val_loss= 0.28235 val_acc= 0.90698 time= 5.29634\n",
      "Epoch: 0149 train_loss= 0.19070 train_acc= 0.94820 val_loss= 0.28073 val_acc= 0.90698 time= 5.22467\n",
      "Epoch: 0150 train_loss= 0.18821 train_acc= 0.94951 val_loss= 0.27914 val_acc= 0.90803 time= 5.15568\n",
      "Epoch: 0151 train_loss= 0.18577 train_acc= 0.95010 val_loss= 0.27760 val_acc= 0.90803 time= 5.51617\n",
      "Epoch: 0152 train_loss= 0.18337 train_acc= 0.95117 val_loss= 0.27610 val_acc= 0.90909 time= 5.78379\n",
      "Epoch: 0153 train_loss= 0.18102 train_acc= 0.95176 val_loss= 0.27463 val_acc= 0.90909 time= 5.95646\n",
      "Epoch: 0154 train_loss= 0.17870 train_acc= 0.95224 val_loss= 0.27321 val_acc= 0.90909 time= 6.20033\n",
      "Epoch: 0155 train_loss= 0.17643 train_acc= 0.95283 val_loss= 0.27182 val_acc= 0.90909 time= 6.00447\n",
      "Epoch: 0156 train_loss= 0.17420 train_acc= 0.95414 val_loss= 0.27047 val_acc= 0.90909 time= 6.10666\n",
      "Epoch: 0157 train_loss= 0.17201 train_acc= 0.95473 val_loss= 0.26915 val_acc= 0.90909 time= 6.11133\n",
      "Epoch: 0158 train_loss= 0.16986 train_acc= 0.95568 val_loss= 0.26786 val_acc= 0.90909 time= 5.97948\n",
      "Epoch: 0159 train_loss= 0.16774 train_acc= 0.95640 val_loss= 0.26662 val_acc= 0.90909 time= 6.01198\n",
      "Epoch: 0160 train_loss= 0.16566 train_acc= 0.95687 val_loss= 0.26542 val_acc= 0.90909 time= 6.19506\n",
      "Epoch: 0161 train_loss= 0.16362 train_acc= 0.95758 val_loss= 0.26425 val_acc= 0.90803 time= 6.14633\n",
      "Epoch: 0162 train_loss= 0.16162 train_acc= 0.95842 val_loss= 0.26311 val_acc= 0.90803 time= 6.32130\n",
      "Epoch: 0163 train_loss= 0.15965 train_acc= 0.95889 val_loss= 0.26199 val_acc= 0.90803 time= 6.18768\n",
      "Epoch: 0164 train_loss= 0.15772 train_acc= 0.95925 val_loss= 0.26091 val_acc= 0.90803 time= 5.33464\n",
      "Epoch: 0165 train_loss= 0.15582 train_acc= 0.95937 val_loss= 0.25986 val_acc= 0.90909 time= 5.18139\n",
      "Epoch: 0166 train_loss= 0.15395 train_acc= 0.96044 val_loss= 0.25885 val_acc= 0.90909 time= 5.13451\n",
      "Epoch: 0167 train_loss= 0.15211 train_acc= 0.96103 val_loss= 0.25786 val_acc= 0.90909 time= 5.16410\n",
      "Epoch: 0168 train_loss= 0.15031 train_acc= 0.96127 val_loss= 0.25691 val_acc= 0.90909 time= 5.09147\n",
      "Epoch: 0169 train_loss= 0.14853 train_acc= 0.96186 val_loss= 0.25598 val_acc= 0.90909 time= 5.19881\n",
      "Epoch: 0170 train_loss= 0.14679 train_acc= 0.96234 val_loss= 0.25509 val_acc= 0.90909 time= 5.11213\n",
      "Epoch: 0171 train_loss= 0.14507 train_acc= 0.96293 val_loss= 0.25422 val_acc= 0.90909 time= 5.12251\n",
      "Epoch: 0172 train_loss= 0.14339 train_acc= 0.96317 val_loss= 0.25338 val_acc= 0.90909 time= 5.12327\n",
      "Epoch: 0173 train_loss= 0.14173 train_acc= 0.96352 val_loss= 0.25256 val_acc= 0.90909 time= 5.54493\n",
      "Epoch: 0174 train_loss= 0.14010 train_acc= 0.96448 val_loss= 0.25177 val_acc= 0.90909 time= 5.72551\n",
      "Epoch: 0175 train_loss= 0.13849 train_acc= 0.96471 val_loss= 0.25100 val_acc= 0.90909 time= 5.15271\n",
      "Epoch: 0176 train_loss= 0.13692 train_acc= 0.96519 val_loss= 0.25026 val_acc= 0.91015 time= 5.10867\n",
      "Epoch: 0177 train_loss= 0.13536 train_acc= 0.96590 val_loss= 0.24954 val_acc= 0.91015 time= 5.21513\n",
      "Epoch: 0178 train_loss= 0.13383 train_acc= 0.96602 val_loss= 0.24884 val_acc= 0.91015 time= 5.20255\n",
      "Epoch: 0179 train_loss= 0.13233 train_acc= 0.96685 val_loss= 0.24816 val_acc= 0.91015 time= 5.16064\n",
      "Epoch: 0180 train_loss= 0.13085 train_acc= 0.96780 val_loss= 0.24750 val_acc= 0.91015 time= 5.08645\n",
      "Epoch: 0181 train_loss= 0.12939 train_acc= 0.96840 val_loss= 0.24685 val_acc= 0.91015 time= 5.16868\n",
      "Epoch: 0182 train_loss= 0.12795 train_acc= 0.96911 val_loss= 0.24624 val_acc= 0.91015 time= 5.14989\n",
      "Epoch: 0183 train_loss= 0.12654 train_acc= 0.96958 val_loss= 0.24564 val_acc= 0.91015 time= 5.17709\n",
      "Epoch: 0184 train_loss= 0.12515 train_acc= 0.97006 val_loss= 0.24507 val_acc= 0.91015 time= 5.16630\n",
      "Epoch: 0185 train_loss= 0.12378 train_acc= 0.97065 val_loss= 0.24451 val_acc= 0.90909 time= 5.14145\n",
      "Epoch: 0186 train_loss= 0.12243 train_acc= 0.97053 val_loss= 0.24397 val_acc= 0.90909 time= 5.16144\n",
      "Epoch: 0187 train_loss= 0.12110 train_acc= 0.97148 val_loss= 0.24346 val_acc= 0.91015 time= 5.15289\n",
      "Epoch: 0188 train_loss= 0.11979 train_acc= 0.97232 val_loss= 0.24296 val_acc= 0.91015 time= 5.19515\n",
      "Epoch: 0189 train_loss= 0.11850 train_acc= 0.97255 val_loss= 0.24248 val_acc= 0.91015 time= 5.14383\n",
      "Epoch: 0190 train_loss= 0.11723 train_acc= 0.97303 val_loss= 0.24202 val_acc= 0.91015 time= 5.11964\n",
      "Epoch: 0191 train_loss= 0.11598 train_acc= 0.97327 val_loss= 0.24157 val_acc= 0.91121 time= 5.13739\n",
      "Epoch: 0192 train_loss= 0.11475 train_acc= 0.97374 val_loss= 0.24115 val_acc= 0.91121 time= 5.14300\n",
      "Epoch: 0193 train_loss= 0.11353 train_acc= 0.97398 val_loss= 0.24074 val_acc= 0.91121 time= 5.16888\n",
      "Epoch: 0194 train_loss= 0.11234 train_acc= 0.97422 val_loss= 0.24034 val_acc= 0.91015 time= 5.11930\n",
      "Epoch: 0195 train_loss= 0.11116 train_acc= 0.97493 val_loss= 0.23996 val_acc= 0.91015 time= 5.16298\n",
      "Epoch: 0196 train_loss= 0.10999 train_acc= 0.97564 val_loss= 0.23960 val_acc= 0.91015 time= 5.10885\n",
      "Epoch: 0197 train_loss= 0.10885 train_acc= 0.97588 val_loss= 0.23926 val_acc= 0.91121 time= 5.14574\n",
      "Epoch: 0198 train_loss= 0.10772 train_acc= 0.97612 val_loss= 0.23893 val_acc= 0.91121 time= 5.08364\n",
      "Epoch: 0199 train_loss= 0.10660 train_acc= 0.97624 val_loss= 0.23862 val_acc= 0.91332 time= 5.13148\n",
      "Epoch: 0200 train_loss= 0.10551 train_acc= 0.97647 val_loss= 0.23832 val_acc= 0.91332 time= 5.12833\n",
      "Finished Training....\n"
     ]
    }
   ],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_accuracy = []\n",
    "train_loss = []\n",
    "val_accuracy = []\n",
    "val_loss = []\n",
    "test_accuracy = []\n",
    "test_loss = []\n",
    "\n",
    "# Train model\n",
    "\n",
    "#configurate checkpoint directory to save intermediate model training weights\n",
    "saver = tf.train.Saver()\n",
    "save_dir = str(cur_dir.joinpath(f'{exp_id}_checkpoints/'))\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "save_path = os.path.join(save_dir, 'best_validation')\n",
    "\n",
    "for epoch in range(FLAGS.epochs):\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, labels_binary_train,\n",
    "                                    train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy],\n",
    "                    feed_dict=feed_dict)\n",
    "    train_accuracy.append(outs[2])\n",
    "    train_loss.append(outs[1])\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, labels_binary_val,\n",
    "                                   val_mask, placeholders)\n",
    "    val_loss.append(cost)\n",
    "    val_accuracy.append(acc)\n",
    "    test_cost, test_acc, test_duration = evaluate(features, support,\n",
    "                                                  labels_binary_test,\n",
    "                                                  test_mask, placeholders)\n",
    "    test_accuracy.append(test_acc)\n",
    "    test_loss.append(test_cost)\n",
    "    saver.save(sess=sess, save_path=save_path)\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\",\n",
    "          \"{:.5f}\".format(outs[1]), \"train_acc=\", \"{:.5f}\".format(outs[2]),\n",
    "          \"val_loss=\", \"{:.5f}\".format(cost), \"val_acc=\", \"{:.5f}\".format(acc),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "    if epoch > FLAGS.early_stopping and val_loss[-1] > np.mean(\n",
    "            val_loss[-(FLAGS.early_stopping + 1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Finished Training....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking train/test/val set accuracy: 0.9768326006890816, 0.9199237368922784, 0.9133192389006343\n",
      "Checking pred set accuracy: 0.8545908567038033\n"
     ]
    }
   ],
   "source": [
    "all_mask = np.array([True] * len(train_mask))\n",
    "labels_binary_all = new_label\n",
    "\n",
    "feed_dict_all = construct_feed_dict(features, support, labels_binary_all,\n",
    "                                    all_mask, placeholders)\n",
    "feed_dict_all.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "activation_output = sess.run(model.activations, feed_dict=feed_dict_all)[1]\n",
    "predict_output = sess.run(model.outputs, feed_dict=feed_dict_all)\n",
    "\n",
    "#' accuracy on all masks\n",
    "ab = sess.run(tf.nn.softmax(predict_output))\n",
    "all_prediction = sess.run(\n",
    "    tf.equal(sess.run(tf.argmax(ab, 1)),\n",
    "             sess.run(tf.argmax(labels_binary_all.astype(\"int32\"), 1))))\n",
    "\n",
    "#' accuracy on prediction masks \n",
    "acc_train = np.sum(all_prediction[train_mask]) / np.sum(train_mask)\n",
    "acc_test = np.sum(all_prediction[test_mask]) / np.sum(test_mask)\n",
    "acc_val = np.sum(all_prediction[val_mask]) / np.sum(val_mask)\n",
    "acc_pred = np.sum(all_prediction[pred_mask]) / np.sum(pred_mask)\n",
    "print('Checking train/test/val set accuracy: {}, {}, {}'.format(\n",
    "    acc_train, acc_test, acc_val))\n",
    "print('Checking pred set accuracy: {}'.format(acc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8545908567038033, 0.9768326006890816, 0.9133192389006343)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_pred, acc_train, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_label = pd.read_csv(join(FLAGS.dataset, 'Label1.csv'))['type'].values\n",
    "tgt_label = pd.read_csv(join(FLAGS.dataset, 'Label2.csv'))['type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_shr_mask = np.in1d(tgt_label, np.unique(src_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pr = np.argmax(ab, axis=1)\n",
    "all_gt = np.argmax(labels_binary_all.A, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8545908567038033"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_pr[pred_mask][tgt_shr_mask] == all_gt[pred_mask][tgt_shr_mask]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import osr_evaluator\n",
    "\n",
    "E_score = pd.read_csv(join(FLAGS.dataset, 'sample_E_score.csv'))\n",
    "H_score = pd.read_csv(join(FLAGS.dataset, 'sample_H_score.csv'))\n",
    "\n",
    "E_score = E_score.x.replace(float('-inf'), 0).values\n",
    "H_score = H_score.x.replace(float('-inf'), 0).values\n",
    "\n",
    "open_score = H_score - E_score\n",
    "\n",
    "kn_data_pr = all_pr[pred_mask][tgt_shr_mask]\n",
    "kn_data_gt = all_gt[pred_mask][tgt_shr_mask]\n",
    "kn_data_open_score = open_score[tgt_shr_mask]\n",
    "\n",
    "unk_data_open_score = open_score[np.logical_not(tgt_shr_mask)]\n",
    "\n",
    "closed_acc, os_auroc, os_aupr, oscr = osr_evaluator(kn_data_pr, kn_data_gt, kn_data_open_score, unk_data_open_score)\n",
    "closed_acc, os_auroc, os_aupr, oscr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf113",
   "language": "python",
   "name": "tf113"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
