import torch
import numpy as np

from torch import nn
import torch.nn.functional as F
from torch.autograd import Variable

epsilon = 1e-5


class MultiSimilarityLoss(nn.Module):
    def __init__(self, args):
        super(MultiSimilarityLoss, self).__init__()
        self.thresh = 0.5
        self.margin = 0.1

        self.scale_pos = args.LOSSES.MULTI_SIMILARITY_LOSS.SCALE_POS
        self.scale_neg = args.LOSSES.MULTI_SIMILARITY_LOSS.SCALE_NEG

    def forward(self, feats, labels):
        assert feats.size(0) == labels.size(0), \
            f"feats.size(0): {feats.size(0)} is not equal to labels.size(0): {labels.size(0)}"
        batch_size = feats.size(0)
        sim_mat = torch.matmul(feats, torch.t(feats))

        epsilon = 1e-5
        loss = list()

        for i in range(batch_size):
            pos_pair_ = sim_mat[i][labels == labels[i]]
            pos_pair_ = pos_pair_[pos_pair_ < 1 - epsilon]
            neg_pair_ = sim_mat[i][labels != labels[i]]

            neg_pair = neg_pair_[neg_pair_ + self.margin > min(pos_pair_)]
            pos_pair = pos_pair_[pos_pair_ - self.margin < max(neg_pair_)]

            if len(neg_pair) < 1 or len(pos_pair) < 1:
                continue

            # weighting step
            pos_loss = 1.0 / self.scale_pos * torch.log(
                1 + torch.sum(torch.exp(-self.scale_pos * (pos_pair - self.thresh))))
            neg_loss = 1.0 / self.scale_neg * torch.log(
                1 + torch.sum(torch.exp(self.scale_neg * (neg_pair - self.thresh))))
            loss.append(pos_loss + neg_loss)

        if len(loss) == 0:
            return torch.zeros([], requires_grad=True)

        loss = sum(loss) / batch_size
        return loss


class MultiSimilarityLoss_GA(nn.Module):
    def __init__(self, args):
        super(MultiSimilarityLoss_GA, self).__init__()
        self.thresh = 0.5
        self.margin = 0.1
        self.args=args
        self.scale_pos = args.MULTI_SIMILARITY_LOSS_SCALE_POS
        self.scale_neg = args.MULTI_SIMILARITY_LOSS_SCALE_NEG

        self.pos_mask = self.mask_correlate(args)
        self.neg_mask = torch.logical_not(pos_mask)

    def mask_correlate(self, args):
 
        block = torch.ones((args['NN_COUNT'], args['NN_COUNT']), dtype=torch.long)
        block = block.unsqueeze(0)
        block = torch.repeat_interleave(block, args['batch_size'], dim=0)

        mask = torch.block_diag(*block)
        mask = mask.bool()

        return mask

    def forward(self, feats1, feats2):
        """Override forward functions, utilizing for loops to compute loss sample by sample.
           Under unsupervised scenario the label is generated by memroyBnak using pseudo-labels.
           
        Args:
            feats (_type_): Feature vectors from porjection head of the network
            labels (_type_): Lables of samples. 

        Returns:
            _type_: Loss
        """
        batch_size = feats1.size(0)
        sim_mat = torch.matmul(feats1, torch.t(feats2))
        epsilon = 1e-5

        pos_mask1 = self.pos_mask.logical_and(sim_mat < (1-epsilon))
        sim_pos1 = torch.where(pos_mask1, sim_mat, torch.ones_like(sim_mat).cuda() * float('inf'))
        sim_neg  = torch.where(self.neg_mask, sim_mat, torch.ones_like(sim_mat).cuda() * float('-inf'))

        pos_min, _ = torch.min(sim_pos1, dim=1, keepdim=True)
        neg_max, _ = torch.max(sim_neg,  dim=1, keepdim=True)

        pos_mask2 = pos_mask1.logical_and((sim_mat - self.margin) < neg_max)
        neg_mask1 = self.neg_mask.logical_and((sim_mat + self.margin) > pos_min)

        # sim_pos2 = torch.where(pos_mask2, sim_mat, float('inf'))
        # sim_neg1 = torch.where(neg_mask1, sim_mat, float('-inf'))

        pos_loss = 1./ self.scale_pos * torch.log(
            1. + torch.sum(torch.exp(-self.scale_pos * (sim_mat - self.thresh)) * pos_mask2, dim=1))
        neg_loss = 1./ self.scale_neg * torch.log(
            1. + torch.sum(torch.exp(self.scale_neg * (sim_mat - self.thresh)) * neg_mask1, dim=1))  # ensuring feat is L2-normed

        null_loss_mask = (pos_mask2.sum(dim=1) >= 1).logical_and(neg_mask1.sum(dim=1) >= 1)
        n_null = null_loss_mask.sum()
        
        if n_null < 1:
            return torch.zeros([], requires_grad=True) 

        loss = (pos_loss + neg_loss).sum() / batch_size
        return loss


class InfoNCE(torch.nn.Module):
    def __init__(self, batch_size, temperature=0.5):
        super().__init__()
        self.batch_size = batch_size
        self.register_buffer("temperature", torch.tensor(temperature))
        # self.temperature = temperature
        self.register_buffer("negatives_mask", (~torch.eye(batch_size * 2, batch_size * 2, 
                                                           dtype=bool)).float())
            
    def forward(self, feat1, feat2):
        """
        emb_i and emb_j are batches of embeddings, where corresponding indices are pairs
        z_i, z_j as per SimCLR paper
        """
        #z_i = F.normalize(emb_i, dim=1)
        #z_j = F.normalize(emb_j, dim=1)
        
        # z_i = F.normalize(emb_i, dim=1,p=2)
        # z_j = F.normalize(emb_j, dim=1,p=2)

        features = torch.cat([feat1, feat2], dim=0)
        representations = F.normalize(features, dim=1, p=2)

        similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)
        
        sim_ij = torch.diag(similarity_matrix, self.batch_size)
        sim_ji = torch.diag(similarity_matrix, -self.batch_size)
        positives = torch.cat([sim_ij, sim_ji], dim=0)
        
        nominator = torch.exp(positives / self.temperature)

        # negatives_mask = ~torch.eye(batch_size * 2, batch_size * 2, dtype=bool).float().cuda()
        denominator = self.negatives_mask * torch.exp(similarity_matrix / self.temperature)
    
        loss_partial = -torch.log(nominator / torch.sum(denominator, dim=1))
        loss = torch.sum(loss_partial) / (2 * self.batch_size)
        return loss

class FocalLoss(nn.Module):
    def __init__(self, gamma=0, alpha=None, size_average=True):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha
        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])
        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)
        self.size_average = size_average

    def forward(self, input, target):
        if input.dim()>2:
            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W
            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C
            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C
        target = target.view(-1,1)

        logpt = F.log_softmax(input, dim=1)
        logpt = logpt.gather(1,target)
        logpt = logpt.view(-1)
        pt = Variable(logpt.data.exp())

        if self.alpha is not None:
            if self.alpha.type()!=input.data.type():
                self.alpha = self.alpha.type_as(input.data)
            at = self.alpha.gather(0,target.data.view(-1))
            logpt = logpt * Variable(at)

        loss = -1 * (1-pt)**self.gamma * logpt
        if self.size_average: return loss.mean()
        else: return loss.sum()


class L1regularization(nn.Module):
    def __init__(self, weight_decay=0.1):
        super(L1regularization, self).__init__()
        self.weight_decay = weight_decay

    def forward(self, model):
        regularization_loss = 0.
        for param in model.parameters():
            regularization_loss += torch.mean(abs(param)) * self.weight_decay

        return regularization_loss


class CrossEntropyLabelSmooth(nn.Module):
    """Cross entropy loss with label smoothing regularizer.
    Reference:
    Szegedy et al. Rethinking the Inception Architecture for Computer Vision. CVPR 2016.
    Equation: y = (1 - epsilon) * y + epsilon / K.
    Args:
        num_classes (int): number of classes.
        epsilon (float): weight.
    """

    def __init__(self, num_classes, epsilon=0.1, reduction=True):
        super(CrossEntropyLabelSmooth, self).__init__()
        self.num_classes = num_classes
        self.epsilon = epsilon
        self.reduction = reduction
        self.logsoftmax = nn.LogSoftmax(dim=1)

    def forward(self, inputs, targets):
        """
        Args:
            inputs: prediction matrix (before softmax) with shape (batch_size, num_classes)
            targets: ground truth labels with shape (num_classes)
        """
        log_probs = self.logsoftmax(inputs)
        targets = torch.zeros(log_probs.size()).cuda().scatter_(1, targets.unsqueeze(1), 1)
        targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes
        loss = (- targets * log_probs).sum(dim=1)
        if self.reduction:
            return loss.mean()
        else:
            return loss
        return loss

import math
def one_hot(label, num_classes):
    onehot = torch.zeros(label.size(0), num_classes).cuda().scatter_(1, label.unsqueeze(1), 1)
    return onehot

class CosineHead(nn.Module):
    def __init__(self, in_features, out_features, scale_factor=30.0):
        super(CosineHead, self).__init__()
        self.scale_factor = scale_factor
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features).float())
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, feature):
        cosine = F.linear(F.normalize(feature), F.normalize(self.weight))
        return cosine * self.scale_factor


class CosineMarginHead(nn.Module):
    """Implement of large margin cosine distance: :
    Args:
        in_features: size of each input sample
        out_features: size of each output sample
        scale_factor: norm of input feature
        margin: margin
    :returnï¼š (theta) - m
    """

    def __init__(self, in_features, out_features, scale_factor=30.0, margin=0.40):
        super(CosineMarginHead, self).__init__()
        self.scale_factor = scale_factor
        self.margin = margin
        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, feature, label=None):
        # --------------------------- cos(theta) & phi(theta) ---------------------------
        cosine = F.linear(F.normalize(feature), F.normalize(self.weight))

        # when test, no label, just return
        if label is None:
            return cosine * self.scale_factor

        phi = cosine - self.margin
        output = torch.where(
            one_hot(label, cosine.shape[1]).byte(), phi, cosine)
        output *= self.scale_factor

        return output


class SoftmaxMarginHead(nn.Module):
    """Implement of softmax with margin:
    Args:
        in_features: size of each input sample
        out_features: size of each output sample
        scale_factor: norm of input feature
        margin: margin
    """

    def __init__(self, in_features, out_features, scale_factor=5.0, margin=0.40):
        super(SoftmaxMarginHead, self).__init__()
        self.scale_factor = scale_factor
        self.margin = margin
        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, feature, label=None):
        z = F.linear(feature, self.weight)
        z -= z.min(dim=1, keepdim=True)[0]
        # when test, no label, just return
        if label is None:
            return z * self.scale_factor

        phi = z - self.margin
        output = torch.where(
            one_hot(label, z.shape[1]).byte(), phi, z)
        output *= self.scale_factor

        return output
